<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#53d361;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#53d361;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-09-20 03:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250920_0321</div>
    <div class="row"><div class="card">
<div class="title">Geometric Image Synchronization with Deep Watermarking</div>
<div class="meta-line">Authors: Pierre Fernandez, Tomáš Souček, Nikola Jovanović, Hady Elsahar, Sylvestre-Alvise Rebuffi, Valeriu Lacatusu, Tuan Tran, Alexandre Mourachko</div>
<div class="meta-line">First: 2025-09-18T17:56:54+00:00 · Latest: 2025-09-18T17:56:54+00:00</div>
<div class="meta-line">Comments: Pre-print. Code at:
  https://github.com/facebookresearch/wmar/tree/main/syncseal</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.15208v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.15208v1">PDF</a> · <a href="https://github.com/facebookresearch/wmar/tree/main/syncseal">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Synchronization is the task of estimating and inverting geometric
transformations (e.g., crop, rotation) applied to an image. This work
introduces SyncSeal, a bespoke watermarking method for robust image
synchronization, which can be applied on top of existing watermarking methods
to enhance their robustness against geometric transformations. It relies on an
embedder network that imperceptibly alters images and an extractor network that
predicts the geometric transformation to which the image was subjected. Both
networks are end-to-end trained to minimize the error between the predicted and
ground-truth parameters of the transformation, combined with a discriminator to
maintain high perceptual quality. We experimentally validate our method on a
wide variety of geometric and valuemetric transformations, demonstrating its
effectiveness in accurately synchronizing images. We further show that our
synchronization can effectively upgrade existing watermarking methods to
withstand geometric transformations to which they were previously vulnerable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>几何图像同步与深度水印技术</div>
<div class="mono" style="margin-top:8px">同步任务旨在估计并逆转图像所经历的几何变换（如裁剪、旋转）。本研究提出了SyncSeal——一种专为鲁棒图像同步定制的数字水印方法，可叠加于现有水印技术上以增强其抗几何变换能力。该方法包含一个不可感知地修改图像的嵌入网络和一个预测图像所受几何变换的提取网络，两者通过端到端训练最小化变换参数预测值与真实值之间的误差，并结合判别器保持高感知质量。我们通过大量几何与数值变换实验验证了该方法在精确图像同步方面的有效性，并证明其能有效提升现有水印技术的抗几何攻击能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of estimating and inverting geometric transformations applied to images, such as cropping and rotation, which is critical for robust image watermarking. The proposed method, SyncSeal, employs an end-to-end trained embedder network to imperceptibly alter images and an extractor network to predict transformation parameters, augmented by a discriminator to preserve perceptual quality. Experimental results demonstrate that SyncSeal accurately synchronizes images under various geometric and valuemetric transformations and significantly enhances the robustness of existing watermarking methods against previously vulnerable geometric attacks.</div>
<div class="mono" style="margin-top:8px">本研究针对图像几何同步的鲁棒性挑战，即估计并反转裁剪、旋转等几何变换。提出的SyncSeal方法采用端到端可训练框架，包含一个不可感知地修改图像的嵌入网络和一个预测变换参数的提取网络，并通过判别器保持感知质量。实验结果表明，SyncSeal在多种几何和数值变换下能准确同步图像，并能有效提升现有水印方法对抗先前易受攻击的几何变换的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Orion: Fuzzing Workflow Automation</div>
<div class="meta-line">Authors: Max Bazalii, Marius Fleischer</div>
<div class="meta-line">First: 2025-09-18T17:52:06+00:00 · Latest: 2025-09-18T17:52:06+00:00</div>
<div class="meta-line">Comments: 11 pages, 3 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.15195v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.15195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fuzz testing is one of the most effective techniques for finding software
vulnerabilities. While modern fuzzers can generate inputs and monitor
executions automatically, the overall workflow, from analyzing a codebase, to
configuring harnesses, to triaging results, still requires substantial manual
effort. Prior attempts focused on single stages such as harness synthesis or
input minimization, leaving researchers to manually connect the pieces into a
complete fuzzing campaign.
  We introduce Orion, a framework that automates the the manual bottlenecks of
fuzzing by integrating LLM reasoning with traditional tools, allowing campaigns
to scale to settings where human effort alone was impractical. Orion uses LLMs
for code reasoning and semantic guidance, while relying on deterministic tools
for verification, iterative refinement, and tasks that require precision.
Across our benchmark suite, Orion reduces human effort by 46-204x depending on
the workflow stage, and we demonstrate its effectiveness through the discovery
of two previously unknown vulnerabilities in the widely used open-source clib
library.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>猎户座：模糊测试工作流自动化</div>
<div class="mono" style="margin-top:8px">模糊测试是发现软件漏洞最有效的技术之一。虽然现代模糊器能自动生成输入并监控执行，但从分析代码库到配置测试框架，再到结果分类的整个工作流仍需大量人工操作。先前的研究集中于单个阶段（如测试框架合成或输入最小化），迫使研究人员手动将各环节拼接成完整的模糊测试活动。我们推出猎户座框架，通过将LLM推理与传统工具结合，自动化处理模糊测试中的人工瓶颈，使测试活动能够扩展到仅靠人力难以应对的场景。猎户座利用LLM进行代码推理和语义指导，同时依赖确定性工具进行验证、迭代优化及需要精确度的任务。在我们的基准测试中，猎户座根据不同工作流阶段将人工投入减少46至204倍，并通过在广泛使用的开源clib库中发现两个前所未知的漏洞证明了其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Fuzz testing is highly effective for vulnerability detection but requires substantial manual effort in workflow stages like harness configuration and result triaging, as prior tools only automated isolated parts. Orion addresses this by integrating LLM reasoning for code analysis and semantic guidance with deterministic tools for verification and refinement, creating a fully automated fuzzing framework. Experimental results show Orion reduces human effort by 46-204x across workflow stages and successfully discovered two previously unknown vulnerabilities in the open-source clib library.</div>
<div class="mono" style="margin-top:8px">模糊测试虽能有效检测漏洞，但在代码库分析、测试配置和结果分类等流程中仍需大量人工操作。Orion通过结合大语言模型的代码推理与语义引导能力，以及确定性工具的验证和优化功能，实现了工作流自动化。实验表明，Orion在不同阶段将人工工作量减少了46至204倍，并在开源clib库中发现了两个先前未知的漏洞。</div>
</details>
</div>
<div class="card">
<div class="title">Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based   Agentic System</div>
<div class="meta-line">Authors: Yuchong Xie, Mingyu Luo, Zesen Liu, Zhixiang Zhang, Kaikai Zhang, Yu Liu, Zongjie Li, Ping Chen, Shuai Wang, Dongdong She</div>
<div class="meta-line">First: 2025-09-06T15:48:49+00:00 · Latest: 2025-09-18T17:38:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.05755v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.05755v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based agentic systems leverage large language models to handle user
queries, make decisions, and execute external tools for complex tasks across
domains like chatbots, customer service, and software engineering. A critical
component of these systems is the Tool Invocation Prompt (TIP), which defines
tool interaction protocols and guides LLMs to ensure the security and
correctness of tool usage. Despite its importance, TIP security has been
largely overlooked. This work investigates TIP-related security risks,
revealing that major LLM-based systems like Cursor, Claude Code, and others are
vulnerable to attacks such as remote code execution (RCE) and denial of service
(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate
external tool behavior hijacking via manipulated tool invocations. We also
propose defense mechanisms to enhance TIP security in LLM-based agentic
systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于LLM的代理系统中工具行为劫持的漏洞利用工具调用提示</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的代理系统利用大模型处理用户查询、做出决策并执行外部工具，以完成跨领域复杂任务，如聊天机器人、客户服务和软件工程。这些系统的关键组件是工具调用提示（TIP），它定义了工具交互协议并指导LLM确保工具使用的安全性和正确性。尽管其重要性，TIP的安全性长期被忽视。本研究探讨了与TIP相关的安全风险，揭示出如Cursor、Claude Code等主流LLM系统易受远程代码执行（RCE）和拒绝服务（DoS）等攻击。通过系统化的TIP利用工作流（TEW），我们演示了通过操纵工具调用实现的外部工具行为劫持，并提出了增强LLM代理系统中TIP安全性的防御机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the critical yet overlooked security vulnerabilities in Tool Invocation Prompts (TIPs), which guide large language models (LLMs) in agentic systems to interact with external tools safely. The authors introduce a systematic TIP exploitation workflow (TEW) to demonstrate how manipulated tool invocations can hijack tool behavior, leading to risks such as remote code execution and denial of service. Experimental findings reveal that major LLM-based systems, including Cursor and Claude Code, are susceptible to these attacks, prompting the proposal of defense mechanisms to enhance TIP security.</div>
<div class="mono" style="margin-top:8px">本研究动机源于工具调用提示（TIP）在基于大语言模型的智能体系统中存在关键但被忽视的安全风险，TIP用于指导模型安全地与外部工具交互。作者提出了一种系统性的TIP利用工作流程（TEW），通过操纵工具调用来演示如何劫持工具行为，导致远程代码执行和拒绝服务等漏洞。实验结果表明，Cursor和Claude Code等主流系统易受此类攻击，因此研究提出了防御机制以增强TIP的安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Watermarking and Anomaly Detection in Machine Learning Models for LORA   RF Fingerprinting</div>
<div class="meta-line">Authors: Aarushi Mahajan, Wayne Burleson</div>
<div class="meta-line">Venue: ICASSP</div>
<div class="meta-line">First: 2025-09-18T17:21:33+00:00 · Latest: 2025-09-18T17:21:33+00:00</div>
<div class="meta-line">Comments: IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.15170v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.15170v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Radio frequency fingerprint identification (RFFI) distinguishes wireless
devices by the small variations in their analog circuits, avoiding heavy
cryptographic authentication. While deep learning on spectrograms improves
accuracy, models remain vulnerable to copying, tampering, and evasion. We
present a stronger RFFI system combining watermarking for ownership proof and
anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel
spectrograms, we embed three watermarks: a simple trigger, an adversarially
trained trigger robust to noise and filtering, and a hidden gradient/weight
signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler
(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,
our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,
offering verifiable, tamper-resistant authentication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于LORA射频指纹识别的机器学习模型水印与异常检测</div>
<div class="mono" style="margin-top:8px">射频指纹识别（RFFI）通过模拟电路的微小差异区分无线设备，避免了繁重的加密认证。虽然基于频谱图的深度学习提高了准确性，但模型仍易受复制、篡改和规避攻击。我们提出一种更强的RFFI系统，结合水印技术用于所有权证明和异常检测用于识别可疑输入。使用ResNet-34处理对数梅尔频谱图，我们嵌入三种水印：简单触发器、抗噪声和滤波的对抗训练触发器，以及隐藏梯度/权重签名。采用KL预热和自由比特技术的卷积变分自编码器（VAE）可检测分布外查询。在LoRa数据集上，我们的系统达到94.6%的准确率、98%的水印成功率和0.94的AUROC值，提供可验证、防篡改的认证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enhance the security and trustworthiness of radio frequency fingerprint identification (RFFI) systems, which are vulnerable to model copying, tampering, and adversarial evasion despite their high accuracy. The method integrates ownership verification via multiple watermarking techniques—including simple triggers, adversarially trained robust triggers, and gradient/weight signatures—into a ResNet-34 model processing log-Mel spectrograms, and employs a convolutional variational autoencoder with KL warm-up for anomaly detection to identify suspicious inputs. Experimental results on a LoRa dataset demonstrate strong performance with 94.6% classification accuracy, 98% watermark detection success, and 0.94 AUROC for anomaly detection, providing a verifiable and tamper-resistant authentication solution.</div>
<div class="mono" style="margin-top:8px">本研究针对射频指纹识别（RFFI）系统易受模型复制、篡改和规避攻击的脆弱性问题，旨在提升其安全性和可验证性。方法上结合了水印技术（用于所有权证明）和异常检测（用于识别可疑输入）：采用ResNet-34处理对数梅尔频谱图进行分类，嵌入三种水印（简单触发、对抗训练触发和梯度/权重签名），并利用带KL预热和自由比特的卷积变分自编码器（VAE）检测分布外查询。在LoRa数据集上的实验显示，系统实现了94.6%的认证准确率、98%的水印成功率和0.94的异常检测AUROC，提供了可验证且防篡改的认证能力。</div>
</details>
</div>
<div class="card">
<div class="title">AIP: Subverting Retrieval-Augmented Generation via Adversarial   Instructional Prompt</div>
<div class="meta-line">Authors: Saket S. Chaturvedi, Gaurav Bagwe, Lan Zhang, Xiaoyong Yuan</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-09-18T17:06:53+00:00 · Latest: 2025-09-18T17:06:53+00:00</div>
<div class="meta-line">Comments: Accepted at EMNLP 2025 Conference</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.15159v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.15159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving relevant documents from external sources to improve factual accuracy
and verifiability. However, this reliance introduces new attack surfaces within
the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have
exposed such vulnerabilities, they largely rely on manipulating user queries,
which is often infeasible in practice due to fixed or protected user inputs.
This narrow focus overlooks a more realistic and stealthy vector: instructional
prompts, which are widely reused, publicly shared, and rarely audited. Their
implicit trust makes them a compelling target for adversaries to manipulate RAG
behavior covertly.
  We introduce a novel attack for Adversarial Instructional Prompt (AIP) that
exploits adversarial instructional prompts to manipulate RAG outputs by subtly
altering retrieval behavior. By shifting the attack surface to the
instructional prompts, AIP reveals how trusted yet seemingly benign interface
components can be weaponized to degrade system integrity. The attack is crafted
to achieve three goals: (1) naturalness, to evade user detection; (2) utility,
to encourage use of prompts; and (3) robustness, to remain effective across
diverse query variations. We propose a diverse query generation strategy that
simulates realistic linguistic variation in user queries, enabling the
discovery of prompts that generalize across paraphrases and rephrasings.
Building on this, a genetic algorithm-based joint optimization is developed to
evolve adversarial prompts by balancing attack success, clean-task utility, and
stealthiness. Experimental results show that AIP achieves up to 95.23% ASR
while preserving benign functionality. These findings uncover a critical and
previously overlooked vulnerability in RAG systems, emphasizing the need to
reassess the shared instructional prompts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AIP：通过对抗性指令提示颠覆检索增强生成</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）通过从外部源检索相关文档来增强大型语言模型（LLM），以提高事实准确性和可验证性。然而，这种依赖性在检索管道中引入了新的攻击面，超越了LLM本身。虽然先前的RAG攻击暴露了此类漏洞，但它们主要依赖于操纵用户查询，这在实践中往往不可行，因为用户输入是固定或受保护的。这种狭隘的关注忽略了一个更现实和隐蔽的向量：指令提示，这些提示被广泛重用、公开共享且很少被审计。它们的隐式信任使其成为对手秘密操纵RAG行为的有吸引力的目标。我们引入了一种新颖的对抗性指令提示（AIP）攻击，通过微妙地改变检索行为来利用对抗性指令提示操纵RAG输出。通过将攻击面转移到指令提示，AIP揭示了受信任但看似良性的接口组件如何被武器化以降低系统完整性。该攻击旨在实现三个目标：（1）自然性，以逃避用户检测；（2）实用性，以鼓励使用提示；（3）鲁棒性，以在不同查询变体中保持有效。我们提出了一种多样化的查询生成策略，模拟用户查询中的真实语言变化，从而发现能够泛化到不同表达方式的提示。在此基础上，开发了基于遗传算法的联合优化，通过平衡攻击成功率、清洁任务实用性和隐蔽性来演化对抗性提示。实验结果表明，AIP在保持良性功能的同时，攻击成功率（ASR）高达95.23%。这些发现揭示了RAG系统中一个关键且先前被忽视的漏洞，强调需要重新评估共享的指令提示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerability of Retrieval-Augmented Generation (RAG) systems to attacks via instructional prompts, which are often reused and trusted but rarely audited. The authors propose Adversarial Instructional Prompt (AIP), a method that uses a genetic algorithm to optimize adversarial prompts for manipulating retrieval behavior, ensuring naturalness, utility, and robustness through diverse query generation. Experiments demonstrate that AIP achieves up to 95.23% attack success rate while maintaining benign functionality, revealing a critical and overlooked security risk in RAG systems.</div>
<div class="mono" style="margin-top:8px">本研究针对检索增强生成（RAG）系统的安全漏洞，动机在于先前攻击方法依赖操纵用户查询，而实际中用户输入往往固定或受保护，难以篡改。作者提出对抗性指令提示（AIP）方法，通过微妙修改广泛重用且受信任的指令提示来操纵检索行为。该方法采用多样化查询生成策略和基于遗传算法的联合优化，以生成自然、实用且对查询变化鲁棒的对抗提示。实验结果显示，AIP在保持良性功能的同时，攻击成功率高达95.23%，揭示了RAG系统中一个关键且此前被忽视的脆弱性。</div>
</details>
</div>
<div class="card">
<div class="title">WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model   via Training-Free Guidance</div>
<div class="meta-line">Authors: Chenxi Song, Yanming Yang, Tong Zhao, Ruibo Li, Chi Zhang</div>
<div class="meta-line">First: 2025-09-18T16:40:47+00:00 · Latest: 2025-09-18T16:40:47+00:00</div>
<div class="meta-line">Comments: Project Webpage: https://worldforge-agi.github.io/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.15130v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.15130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://worldforge-agi.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent video diffusion models demonstrate strong potential in spatial
intelligence tasks due to their rich latent world priors. However, this
potential is hindered by their limited controllability and geometric
inconsistency, creating a gap between their strong priors and their practical
use in 3D/4D tasks. As a result, current approaches often rely on retraining or
fine-tuning, which risks degrading pretrained knowledge and incurs high
computational costs. To address this, we propose WorldForge, a training-free,
inference-time framework composed of three tightly coupled modules. Intra-Step
Recursive Refinement introduces a recursive refinement mechanism during
inference, which repeatedly optimizes network predictions within each denoising
step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages
optical flow similarity to decouple motion from appearance in the latent space
and selectively inject trajectory guidance into motion-related channels.
Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths
to adaptively correct trajectory drift caused by noisy or misaligned structural
signals. Together, these components inject fine-grained, trajectory-aligned
guidance without training, achieving both accurate motion control and
photorealistic content generation. Extensive experiments across diverse
benchmarks validate our method&#x27;s superiority in realism, trajectory
consistency, and visual fidelity. This work introduces a novel plug-and-play
paradigm for controllable video synthesis, offering a new perspective on
leveraging generative priors for spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WorldForge：通过免训练引导解锁视频扩散模型中的涌现式3D/4D生成</div>
<div class="mono" style="margin-top:8px">当前视频扩散模型凭借其丰富的潜在世界先验，在空间智能任务中展现出强大潜力。然而，有限的可控性和几何不一致性制约了这种潜力，导致其强大先验与3D/4D任务实际应用之间存在差距。现有方法通常需要重新训练或微调，这不仅可能损害预训练知识，还带来高昂计算成本。为此，我们提出WorldForge——一个由三个紧密耦合模块组成的免训练推理框架：步内递归优化通过在推理时引入递归优化机制，在每个去噪步骤内反复优化网络预测以实现精确轨迹注入；流控潜在融合利用光流相似性解耦潜在空间中的运动与外观，并将轨迹引导选择性地注入运动相关通道；双路径自校正引导通过比较引导与非引导去噪路径，自适应校正由噪声或错位结构信号引起的轨迹漂移。这些组件共同实现了无需训练的细粒度轨迹对齐引导，兼具精确运动控制与逼真内容生成。跨多个基准的广泛实验验证了本方法在真实性、轨迹一致性和视觉保真度方面的优越性。这项工作为可控视频合成引入了新颖的即插即用范式，为利用生成先验实现空间智能提供了新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of video diffusion models in 3D/4D generation, particularly their lack of controllability and geometric inconsistency, which hinder practical application despite strong spatial priors. The proposed WorldForge framework introduces a training-free, inference-time method with three key modules: Intra-Step Recursive Refinement for precise trajectory injection through repeated optimization, Flow-Gated Latent Fusion to decouple motion from appearance using optical flow similarity, and Dual-Path Self-Corrective Guidance to adaptively correct trajectory drift. Experimental results across diverse benchmarks demonstrate superior performance in realism, trajectory consistency, and visual fidelity, establishing a new plug-and-play paradigm for controllable video synthesis.</div>
<div class="mono" style="margin-top:8px">本研究针对视频扩散模型在3D/4D生成中的局限性，特别是其可控性不足和几何不一致性问题，这些问题阻碍了其强大空间先验的实际应用。提出的WorldForge框架采用免训练推理方法，包含三个模块：帧内递归细化实现精确轨迹注入，流门控潜在融合解耦运动与外观引导，以及双路径自校正指导纠正轨迹漂移。在多基准测试中，该方法在真实性、轨迹一致性和视觉保真度方面表现出优越性能，为可控视频合成提供了即插即用的新范式。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250919_2020.html">20250919_2020</a>
<a href="archive/20250919_1953.html">20250919_1953</a>
<a href="archive/20250919_1945.html">20250919_1945</a>
<a href="archive/20250919_1934.html">20250919_1934</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
