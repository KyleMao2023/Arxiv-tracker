<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#53d361;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#53d361;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-09-19 20:20</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250919_2020</div>
    <div class="row"><div class="card">
<div class="title">Geometric Image Synchronization with Deep Watermarking</div>
<div class="meta-line">Authors: Pierre Fernandez, Tomáš Souček, Nikola Jovanović, Hady Elsahar, Sylvestre-Alvise Rebuffi, Valeriu Lacatusu, Tuan Tran, Alexandre Mourachko</div>
<div class="meta-line">First: 2025-09-18T17:56:54+00:00 · Latest: 2025-09-18T17:56:54+00:00</div>
<div class="meta-line">Comments: Pre-print. Code at:
  https://github.com/facebookresearch/wmar/tree/main/syncseal</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.15208v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.15208v1">PDF</a> · <a href="https://github.com/facebookresearch/wmar/tree/main/syncseal">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Synchronization is the task of estimating and inverting geometric
transformations (e.g., crop, rotation) applied to an image. This work
introduces SyncSeal, a bespoke watermarking method for robust image
synchronization, which can be applied on top of existing watermarking methods
to enhance their robustness against geometric transformations. It relies on an
embedder network that imperceptibly alters images and an extractor network that
predicts the geometric transformation to which the image was subjected. Both
networks are end-to-end trained to minimize the error between the predicted and
ground-truth parameters of the transformation, combined with a discriminator to
maintain high perceptual quality. We experimentally validate our method on a
wide variety of geometric and valuemetric transformations, demonstrating its
effectiveness in accurately synchronizing images. We further show that our
synchronization can effectively upgrade existing watermarking methods to
withstand geometric transformations to which they were previously vulnerable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>几何图像同步与深度水印技术</div>
<div class="mono" style="margin-top:8px">同步任务旨在估计并逆转图像所经历的几何变换（如裁剪、旋转）。本研究提出SyncSeal——一种专为鲁棒图像同步定制的数字水印方法，可叠加于现有水印技术上增强其抗几何变换能力。该方法包含不可感知修改图像的嵌入网络和预测图像几何变换的提取网络，两者通过端到端训练最小化变换参数预测误差，并结合判别器保持高感知质量。实验验证表明，该方法能有效应对多种几何与数值度量变换，实现精准图像同步，并能升级现有水印技术以抵御先前易受攻击的几何变换。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of estimating and inverting geometric transformations applied to images, such as cropping and rotation, which is critical for robust image watermarking. The proposed method, SyncSeal, employs an end-to-end trained system consisting of an embedder network that imperceptibly alters images and an extractor network that predicts transformation parameters, augmented by a discriminator to preserve perceptual quality. Experimental results demonstrate that SyncSeal accurately synchronizes images under various geometric and valuemetric transformations and effectively enhances existing watermarking methods to resist previously vulnerable geometric attacks.</div>
<div class="mono" style="margin-top:8px">本研究针对图像几何变换（如裁剪和旋转）的估计与反转问题，这对于鲁棒图像水印至关重要。提出的SyncSeal方法采用端到端可训练框架，包含一个不可感知地修改图像的嵌入网络和一个预测变换参数的提取网络，并辅以判别器保持感知质量。实验结果表明，SyncSeal在各种几何和数值变换下能准确同步图像，并显著提升现有水印方法对几何攻击的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Orion: Fuzzing Workflow Automation</div>
<div class="meta-line">Authors: Max Bazalii, Marius Fleischer</div>
<div class="meta-line">First: 2025-09-18T17:52:06+00:00 · Latest: 2025-09-18T17:52:06+00:00</div>
<div class="meta-line">Comments: 11 pages, 3 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.15195v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.15195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fuzz testing is one of the most effective techniques for finding software
vulnerabilities. While modern fuzzers can generate inputs and monitor
executions automatically, the overall workflow, from analyzing a codebase, to
configuring harnesses, to triaging results, still requires substantial manual
effort. Prior attempts focused on single stages such as harness synthesis or
input minimization, leaving researchers to manually connect the pieces into a
complete fuzzing campaign.
  We introduce Orion, a framework that automates the the manual bottlenecks of
fuzzing by integrating LLM reasoning with traditional tools, allowing campaigns
to scale to settings where human effort alone was impractical. Orion uses LLMs
for code reasoning and semantic guidance, while relying on deterministic tools
for verification, iterative refinement, and tasks that require precision.
Across our benchmark suite, Orion reduces human effort by 46-204x depending on
the workflow stage, and we demonstrate its effectiveness through the discovery
of two previously unknown vulnerabilities in the widely used open-source clib
library.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>猎户座：模糊测试工作流自动化</div>
<div class="mono" style="margin-top:8px">模糊测试是发现软件漏洞最有效的技术之一。虽然现代模糊器能自动生成输入并监控执行过程，但从分析代码库、配置测试工具到结果分类的整个工作流仍需大量人工操作。先前的研究集中于单个阶段（如测试工具合成或输入最小化），迫使研究人员手动将各环节拼接成完整的模糊测试活动。我们推出猎户座框架，通过将LLM推理与传统工具结合，自动化处理模糊测试中的人工瓶颈，使测试活动能够扩展到仅靠人力难以应对的场景。猎户座利用LLM进行代码推理和语义指导，同时依赖确定性工具进行验证、迭代优化及需要精确度的任务。在我们的基准测试套件中，猎户座根据不同工作流阶段将人工工作量减少了46至204倍，并通过在广泛使用的开源clib库中发现两个前所未知的漏洞证明了其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Fuzz testing is highly effective for vulnerability detection but requires significant manual effort in workflow stages like code analysis, harness configuration, and result triaging. Orion automates these bottlenecks by integrating LLM reasoning for code analysis and semantic guidance with deterministic tools for verification and refinement. Experimental results show Orion reduces human effort by 46-204x across workflow stages and successfully discovered two previously unknown vulnerabilities in the open-source clib library.</div>
<div class="mono" style="margin-top:8px">模糊测试是检测漏洞的有效技术，但其工作流程中的配置和结果分析等阶段需要大量人工操作。Orion通过将大语言模型的语义推理和代码分析与传统确定性工具的验证和优化相结合，实现了这些瓶颈的自动化。实验结果表明，Orion在不同工作流程阶段将人工工作量减少了46-204倍，并成功发现了开源clib库中的两个先前未知的漏洞。</div>
</details>
</div>
<div class="card">
<div class="title">Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based   Agentic System</div>
<div class="meta-line">Authors: Yuchong Xie, Mingyu Luo, Zesen Liu, Zhixiang Zhang, Kaikai Zhang, Yu Liu, Zongjie Li, Ping Chen, Shuai Wang, Dongdong She</div>
<div class="meta-line">First: 2025-09-06T15:48:49+00:00 · Latest: 2025-09-18T17:38:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.05755v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.05755v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based agentic systems leverage large language models to handle user
queries, make decisions, and execute external tools for complex tasks across
domains like chatbots, customer service, and software engineering. A critical
component of these systems is the Tool Invocation Prompt (TIP), which defines
tool interaction protocols and guides LLMs to ensure the security and
correctness of tool usage. Despite its importance, TIP security has been
largely overlooked. This work investigates TIP-related security risks,
revealing that major LLM-based systems like Cursor, Claude Code, and others are
vulnerable to attacks such as remote code execution (RCE) and denial of service
(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate
external tool behavior hijacking via manipulated tool invocations. We also
propose defense mechanisms to enhance TIP security in LLM-based agentic
systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于LLM的代理系统中工具行为劫持的漏洞利用工具调用提示</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的代理系统利用大模型处理用户查询、做出决策并执行外部工具，以完成跨领域复杂任务，如聊天机器人、客户服务和软件工程。这些系统的关键组件是工具调用提示（TIP），它定义了工具交互协议并指导LLM确保工具使用的安全性和正确性。尽管其重要性，TIP的安全性长期被忽视。本研究探讨了与TIP相关的安全风险，揭示出如Cursor、Claude Code等主流LLM系统易受远程代码执行（RCE）和拒绝服务（DoS）等攻击。通过系统化的TIP利用工作流（TEW），我们演示了通过操纵工具调用实现的外部工具行为劫持，并提出了增强LLM代理系统中TIP安全性的防御机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the critical yet overlooked security risks associated with Tool Invocation Prompts (TIPs) in LLM-based agentic systems, which are widely used for complex tasks across domains like chatbots and software engineering. The authors introduce a systematic TIP exploitation workflow (TEW) to demonstrate how manipulated tool invocations can hijack external tool behavior, leading to vulnerabilities such as remote code execution and denial of service in major systems like Cursor and Claude Code. Experimental findings confirm these systems&#x27; susceptibility to attacks, and the study proposes defense mechanisms to enhance TIP security.</div>
<div class="mono" style="margin-top:8px">本研究动机源于大型语言模型（LLM）代理系统中工具调用提示（TIP）的关键但被忽视的安全风险，这些系统广泛应用于聊天机器人和软件工程等领域。作者提出了一种系统的TIP利用工作流程（TEW），通过操纵工具调用来演示外部工具行为劫持。实验结果表明，主流系统如Cursor和Claude Code存在漏洞，可导致远程代码执行和拒绝服务等攻击，并提出了增强TIP安全性的防御机制。</div>
</details>
</div>
<div class="card">
<div class="title">Watermarking and Anomaly Detection in Machine Learning Models for LORA   RF Fingerprinting</div>
<div class="meta-line">Authors: Aarushi Mahajan, Wayne Burleson</div>
<div class="meta-line">Venue: ICASSP</div>
<div class="meta-line">First: 2025-09-18T17:21:33+00:00 · Latest: 2025-09-18T17:21:33+00:00</div>
<div class="meta-line">Comments: IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.15170v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.15170v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Radio frequency fingerprint identification (RFFI) distinguishes wireless
devices by the small variations in their analog circuits, avoiding heavy
cryptographic authentication. While deep learning on spectrograms improves
accuracy, models remain vulnerable to copying, tampering, and evasion. We
present a stronger RFFI system combining watermarking for ownership proof and
anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel
spectrograms, we embed three watermarks: a simple trigger, an adversarially
trained trigger robust to noise and filtering, and a hidden gradient/weight
signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler
(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,
our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,
offering verifiable, tamper-resistant authentication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于LORA射频指纹识别的机器学习模型水印与异常检测</div>
<div class="mono" style="margin-top:8px">射频指纹识别（RFFI）通过模拟电路的微小差异区分无线设备，避免了繁重的加密认证。虽然基于频谱图的深度学习提高了准确性，但模型仍易受复制、篡改和规避攻击。我们提出一种更强的RFFI系统，结合水印技术实现所有权证明，并利用异常检测识别可疑输入。采用ResNet-34网络处理对数梅尔频谱图，嵌入三种水印：简单触发器、抗噪声和滤波的对抗训练触发器，以及隐藏梯度/权重签名。通过带有KL预热和自由比特位的卷积变分自编码器（VAE）检测分布外查询。在LoRa数据集上，该系统实现94.6%准确率、98%水印成功率和0.94 AUROC，提供可验证、防篡改的认证机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enhance the security of radio frequency fingerprint identification (RFFI) systems, which are vulnerable to model copying, tampering, and adversarial evasion despite their high authentication accuracy. The proposed method integrates ownership watermarking and input anomaly detection into a ResNet-34 classifier trained on log-Mel spectrograms, using three watermark types—simple trigger, adversarial trigger, and gradient/weight signature—alongside a convolutional variational autoencoder (VAE) with KL warm-up for anomaly screening. Experimental results on a LoRa dataset demonstrate 94.6% authentication accuracy, 98% watermark detection rate, and 0.94 AUROC for anomaly detection, proving effective tamper resistance and verifiability.</div>
<div class="mono" style="margin-top:8px">本研究旨在增强射频指纹识别（RFFI）系统的安全性和可信性，尽管其准确性高，但仍易受模型复制、篡改和对抗性规避攻击。所提出的方法结合了多种水印技术（包括简单触发、对抗性触发和基于梯度的签名）进行所有权保护，并使用卷积变分自编码器进行异常检测以识别可疑输入。在LoRa数据集上的实验结果表明，该系统实现了94.6%的身份验证准确率、98%的水印检测率和0.94的异常检测AUROC，提供了一个可验证且防篡改的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">AIP: Subverting Retrieval-Augmented Generation via Adversarial   Instructional Prompt</div>
<div class="meta-line">Authors: Saket S. Chaturvedi, Gaurav Bagwe, Lan Zhang, Xiaoyong Yuan</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-09-18T17:06:53+00:00 · Latest: 2025-09-18T17:06:53+00:00</div>
<div class="meta-line">Comments: Accepted at EMNLP 2025 Conference</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.15159v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.15159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving relevant documents from external sources to improve factual accuracy
and verifiability. However, this reliance introduces new attack surfaces within
the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have
exposed such vulnerabilities, they largely rely on manipulating user queries,
which is often infeasible in practice due to fixed or protected user inputs.
This narrow focus overlooks a more realistic and stealthy vector: instructional
prompts, which are widely reused, publicly shared, and rarely audited. Their
implicit trust makes them a compelling target for adversaries to manipulate RAG
behavior covertly.
  We introduce a novel attack for Adversarial Instructional Prompt (AIP) that
exploits adversarial instructional prompts to manipulate RAG outputs by subtly
altering retrieval behavior. By shifting the attack surface to the
instructional prompts, AIP reveals how trusted yet seemingly benign interface
components can be weaponized to degrade system integrity. The attack is crafted
to achieve three goals: (1) naturalness, to evade user detection; (2) utility,
to encourage use of prompts; and (3) robustness, to remain effective across
diverse query variations. We propose a diverse query generation strategy that
simulates realistic linguistic variation in user queries, enabling the
discovery of prompts that generalize across paraphrases and rephrasings.
Building on this, a genetic algorithm-based joint optimization is developed to
evolve adversarial prompts by balancing attack success, clean-task utility, and
stealthiness. Experimental results show that AIP achieves up to 95.23% ASR
while preserving benign functionality. These findings uncover a critical and
previously overlooked vulnerability in RAG systems, emphasizing the need to
reassess the shared instructional prompts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AIP：通过对抗性指令提示颠覆检索增强生成</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）通过从外部源检索相关文档来增强大型语言模型（LLM），以提高事实准确性和可验证性。然而，这种依赖性在检索管道中引入了新的攻击面，超越了LLM本身。虽然先前的RAG攻击暴露了此类漏洞，但它们主要依赖于操纵用户查询，这在实践中往往不可行，因为用户输入是固定或受保护的。这种狭隘的关注忽略了一个更现实和隐蔽的向量：指令提示，这些提示被广泛重用、公开共享且很少被审计。它们的隐式信任使其成为对手秘密操纵RAG行为的有吸引力的目标。我们引入了一种新颖的对抗性指令提示（AIP）攻击，通过微妙地改变检索行为来利用对抗性指令提示操纵RAG输出。通过将攻击面转移到指令提示，AIP揭示了受信任但看似良性的接口组件如何被武器化以降低系统完整性。该攻击旨在实现三个目标：（1）自然性，以逃避用户检测；（2）实用性，以鼓励使用提示；（3）鲁棒性，以在不同查询变体中保持有效。我们提出了一种多样化的查询生成策略，模拟用户查询中的真实语言变化，从而发现能够泛化到不同表达方式的提示。在此基础上，开发了一种基于遗传算法的联合优化方法，通过平衡攻击成功率、清洁任务实用性和隐蔽性来演化对抗性提示。实验结果表明，AIP在保持良性功能的同时，攻击成功率高达95.23%。这些发现揭示了RAG系统中一个关键且先前被忽视的漏洞，强调需要重新评估共享指令提示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerability of Retrieval-Augmented Generation (RAG) systems to attacks via instructional prompts, which are often reused and trusted without scrutiny. The authors propose Adversarial Instructional Prompt (AIP), a method that uses a genetic algorithm to optimize adversarial prompts for manipulating retrieval behavior, ensuring naturalness, utility, and robustness through diverse query generation. Experiments demonstrate that AIP achieves up to 95.23% attack success rate while maintaining benign functionality, revealing a critical and overlooked threat in RAG systems.</div>
<div class="mono" style="margin-top:8px">本研究针对检索增强生成（RAG）系统的安全漏洞，动机在于先前攻击依赖于操纵用户查询，而用户查询通常难以修改，却忽略了可重用且很少被审核的指令提示。方法引入了对抗性指令提示（AIP），采用多样化查询生成策略模拟语言变化，并通过基于遗传算法的联合优化来平衡攻击成功率、实用性和隐蔽性，以制作对抗性提示。实验结果表明，AIP在保持良性功能的同时实现了高达95.23%的攻击成功率，揭示了RAG系统中一个关键且先前被忽视的漏洞。</div>
</details>
</div>
<div class="card">
<div class="title">WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model   via Training-Free Guidance</div>
<div class="meta-line">Authors: Chenxi Song, Yanming Yang, Tong Zhao, Ruibo Li, Chi Zhang</div>
<div class="meta-line">First: 2025-09-18T16:40:47+00:00 · Latest: 2025-09-18T16:40:47+00:00</div>
<div class="meta-line">Comments: Project Webpage: https://worldforge-agi.github.io/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.15130v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.15130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://worldforge-agi.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent video diffusion models demonstrate strong potential in spatial
intelligence tasks due to their rich latent world priors. However, this
potential is hindered by their limited controllability and geometric
inconsistency, creating a gap between their strong priors and their practical
use in 3D/4D tasks. As a result, current approaches often rely on retraining or
fine-tuning, which risks degrading pretrained knowledge and incurs high
computational costs. To address this, we propose WorldForge, a training-free,
inference-time framework composed of three tightly coupled modules. Intra-Step
Recursive Refinement introduces a recursive refinement mechanism during
inference, which repeatedly optimizes network predictions within each denoising
step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages
optical flow similarity to decouple motion from appearance in the latent space
and selectively inject trajectory guidance into motion-related channels.
Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths
to adaptively correct trajectory drift caused by noisy or misaligned structural
signals. Together, these components inject fine-grained, trajectory-aligned
guidance without training, achieving both accurate motion control and
photorealistic content generation. Extensive experiments across diverse
benchmarks validate our method&#x27;s superiority in realism, trajectory
consistency, and visual fidelity. This work introduces a novel plug-and-play
paradigm for controllable video synthesis, offering a new perspective on
leveraging generative priors for spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WorldForge：通过免训练引导解锁视频扩散模型中的涌现式3D/4D生成</div>
<div class="mono" style="margin-top:8px">当前视频扩散模型凭借其丰富的潜在世界先验，在空间智能任务中展现出强大潜力。然而，有限的可控性和几何不一致性制约了这种潜力，导致其强大先验与实际3D/4D任务应用之间存在差距。现有方法通常需要重新训练或微调，这不仅可能损害预训练知识，还带来高昂计算成本。为此，我们提出WorldForge——一个由三个紧密耦合模块组成的免训练推理框架：步内递归优化通过在推理时引入递归优化机制，在每个去噪步骤内反复优化网络预测以实现精确轨迹注入；流门控潜在融合利用光流相似性解耦潜在空间中的运动与外观，并将轨迹引导选择性地注入运动相关通道；双路径自校正引导通过比较有引导与无引导的去噪路径，自适应校正由噪声或错位结构信号引起的轨迹漂移。这些组件共同实现了无需训练的细粒度轨迹对齐引导，既确保精确运动控制，又实现逼真内容生成。跨多个基准的广泛实验验证了本方法在真实性、轨迹一致性和视觉保真度方面的优越性。这项工作为可控视频合成引入了新颖的即插即用范式，为利用生成先验实现空间智能提供了新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of video diffusion models in 3D/4D generation, particularly their lack of controllability and geometric inconsistency, which hinder practical application despite strong spatial priors. The proposed WorldForge framework introduces a training-free, inference-time method with three modules: Intra-Step Recursive Refinement for precise trajectory injection, Flow-Gated Latent Fusion to decouple motion and appearance guidance, and Dual-Path Self-Corrective Guidance to adaptively correct trajectory drift. Experimental results across diverse benchmarks demonstrate superior performance in realism, trajectory consistency, and visual fidelity, validating the approach&#x27;s effectiveness for controllable video synthesis without retraining.</div>
<div class="mono" style="margin-top:8px">本研究针对视频扩散模型在3D/4D生成中的可控性不足和几何不一致问题，尽管模型具有强大的空间先验，但这些限制阻碍了实际应用。提出的WorldForge框架采用无需训练的推理时方法，包含三个模块：帧内递归细化实现精确轨迹注入，流门控潜在融合解耦运动与外观引导，以及双路径自校正指导自适应修正轨迹漂移。在多基准测试中，该方法在真实性、轨迹一致性和视觉保真度方面表现出优越性能，为可控视频合成提供了即插即用的新范式。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250919_1953.html">20250919_1953</a>
<a href="archive/20250919_1945.html">20250919_1945</a>
<a href="archive/20250919_1934.html">20250919_1934</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
